{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b740975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Dense, LayerNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc32891",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "\n",
    "Positional embeddings are order or position identifiers added to the initial vector representation of the inputs for the transformer to know the order of sequence.\n",
    "\n",
    "The transformer does not process the inputs sequentially, but in parallel. For each element it combines information from each other elements through self-attention, but each element does this aggregation on its own, independently of what other elements do or have done yet.\n",
    "\n",
    "Because the transformer architecture does not model the order of the input anywhere, we must encode the order of the input explicitly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e4d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d):\n",
    "    return pos  / np.power(10000, (2 * (i//2) / d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de3542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encodings(num_words, embed_dim):\n",
    "    angles_rad = get_angles(pos = np.arange(num_words)[:, np.newaxis], # pos - shape: (num_words, 1)\n",
    "                            i = np.arange(embed_dim)[np.newaxis, :],   # i   - shape: (1, embed_dim)\n",
    "                            d = embed_dim) # scaler\n",
    "\n",
    "\n",
    "    # For even indices, get 'sine' value of angles\n",
    "    angles_rad[:, 0::2] = np.sin(angles_rad[:, 0::2])\n",
    "\n",
    "    # For odd indices, get 'cosine' value of angles\n",
    "    angles_rad[:, 1::2] = np.cos(angles_rad[:, 1::2])\n",
    "\n",
    "    pos_encoding = angles_rad[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    # return angles_rad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7103374",
   "metadata": {},
   "source": [
    "# Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d32b96",
   "metadata": {},
   "source": [
    "**Padding Mask**\n",
    "\n",
    "When passing sequences into a transformer model, it is important that they are of uniform length. This can be achieved by padding the sequences with zeros, and truncating sentences that exceed the maximum length of the model.\n",
    "\n",
    "However, the padded zeros will affect the softmax calculation. In order to ensure that padding doesn’t contribute to the self-attention we use Padding Mask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abb06b6",
   "metadata": {},
   "source": [
    "**Look-ahead Mask**\n",
    "\n",
    "In training, the model has access to the complete correct output of the training example. \n",
    "\n",
    "Therefore, while predicting a word at a certain position, the Decoder has available to it the target words preceding that word as well as the target words following that word. This allows the Decoder to ‘cheat’ by using target words from future ‘time steps’. The Look-ahead Mask is used to mask out input words that appear later in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dcc5230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e617cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd2646f",
   "metadata": {},
   "source": [
    "#  Building Transformer Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06fca5e",
   "metadata": {},
   "source": [
    "### Scaled Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5870c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    # Give each word a score that corresponds to the focus it should put to \n",
    "    # other words in the sequence\n",
    "    score_matrix = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    # Scale down the scores to allow for more stable gradients as multiplying\n",
    "    # values can have explidng effects\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_scores = score_matrix / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_scores += (mask * -1e9)  \n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_scores, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fe869c",
   "metadata": {},
   "source": [
    "### Multi-head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d3839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(concat_attention)\n",
    "            \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143ac69c",
   "metadata": {},
   "source": [
    "### Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6042fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    ffn = tf.keras.Sequential([\n",
    "        Dense(fully_connected_dim, activation='relu'),\n",
    "        Dense(embedding_dim)\n",
    "    ])\n",
    "\n",
    "    return ffn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c63eb",
   "metadata": {},
   "source": [
    "### Transformer Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d2ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model=embedding_dim,\n",
    "                                      num_heads=num_heads)\n",
    "\n",
    "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
    "                                  fully_connected_dim=fully_connected_dim)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "    \n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        # Pass the Q, V, K matrices and a boolean mask to the Multi-head Attention layer.\n",
    "        # To compute self-attention Q, V and K should be the same (x)\n",
    "        self_attn_output, _ = self.mha(x, x, x, mask)\n",
    "        # Apply dropout layer to the self-attention output\n",
    "        self_attn_output = self.dropout1(self_attn_output, training=training)\n",
    "        # Apply layer normalization on sum of the input and the attention output\n",
    "        mult_attn_out = self.layernorm1(x + self_attn_output) \n",
    "        # Pass the output of the multi-head attention layer through a ffn\n",
    "        ffn_output = self.ffn(mult_attn_out) \n",
    "        # Apply dropout layer to ffn output\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        # Apply layer normalization on sum of the output from multi-head attention\n",
    "        # and ffn output to get the output of the encoder layer\n",
    "        encoder_layer_out = self.layernorm2(ffn_output + mult_attn_out)\n",
    "        \n",
    "        return encoder_layer_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae187a2d",
   "metadata": {},
   "source": [
    "### Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f98ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):   \n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = get_positional_encodings(maximum_position_encoding, \n",
    "                                                     self.embedding_dim)\n",
    " \n",
    "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # Pass input through the Embedding layer\n",
    "        x = self.embedding(x)\n",
    "        # Scale embedding by multiplying it by the square root of the embedding dimension\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim,tf.float32))\n",
    "        # Add the position encoding to embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        # Pass the encoded embedding through a dropout layer\n",
    "        x = self.dropout(x, training=training) \n",
    "        # Pass the output through the stack of encoding layers \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca66ba7",
   "metadata": {},
   "source": [
    "### Transformer Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3371f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model=embedding_dim,\n",
    "                                       num_heads=num_heads)\n",
    "\n",
    "        self.mha2 = MultiHeadAttention(d_model=embedding_dim,\n",
    "                                       num_heads=num_heads)\n",
    "\n",
    "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
    "                                  fully_connected_dim=fully_connected_dim)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "        self.dropout3 = Dropout(dropout_rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):       \n",
    "        # Calculate self-attention and return attention scores as attn_weights_block1\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        # Apply dropout layer on the attention output\n",
    "        attn1 = self.dropout1(attn1, training = training)\n",
    "        # Apply layer normalization to the sum of the attention output and the input\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        # Calculate self-attention using the Q from the first block and K and V from \n",
    "        # the encoder output and return attention scores as attn_weights_block2\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)          \n",
    "        # Apply dropout layer on the attention output\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        # Apply layer normalization to the sum of the attention output and the output of the first block \n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        # Pass the output of the second block through a ffn\n",
    "        ffn_output = self.ffn(out2)\n",
    "        # Apply a dropout layer to the ffn output\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        # Apply layer normalization to the sum of the ffn output and the output of the second block\n",
    "        out3 =  self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28610498",
   "metadata": {},
   "source": [
    "### Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cde59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(target_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = get_positional_encodings(maximum_position_encoding, self.embedding_dim)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "                           \n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        # Create word embeddings \n",
    "        x = self.embedding(x)\n",
    "        # Scale embedding by multiplying it by the square root of the embedding dimension\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "        # Calculate positional encodings and add to word embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        # Pass the encoded embeddings through a dropout layer\n",
    "        x = self.dropout(x, training=training)\n",
    "        # Pass the output through a stack of decoder layers and update attention_weights\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            # Update attention_weights dictionary with the attention weights of block 1 and block 2\n",
    "            attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = block2\n",
    "        \n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8959df4a",
   "metadata": {},
   "source": [
    "### Transformer (Encoder + Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29a2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Complete transformer with an Encoder and a Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n",
    "                 target_vocab_size, max_positional_encoding_input, max_positional_encoding_target,\n",
    "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers=num_layers,\n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               input_vocab_size=input_vocab_size,\n",
    "                               maximum_position_encoding=max_positional_encoding_input,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, \n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               target_vocab_size=target_vocab_size, \n",
    "                               maximum_position_encoding=max_positional_encoding_target,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.final_layer = Dense(target_vocab_size, activation='softmax')\n",
    "    \n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
